{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logit_IntentClass.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfNjkV0bktU0",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "\n",
        "Intent classification system based on the Logistic Regression model for restaurant information search.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZoCzg0eKG82",
        "colab_type": "text"
      },
      "source": [
        "## Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W0JKuZuKHXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# SKlearn\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "\n",
        "# Joblib\n",
        "from joblib import dump, load\n",
        "\n",
        "# NumPy\n",
        "import numpy as np\n",
        "\n",
        "# Standard\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUgYtoA0fgz",
        "colab_type": "text"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXaPUP5ssII2",
        "colab_type": "text"
      },
      "source": [
        "The datasets and dictionaries used in this notebook can be downloaded from this [link](https://drive.google.com/drive/folders/178Sv30P-OvoJc_QOOAYkmUHgcQsPg-rb?usp=sharing). Also, it will be necessary to change the path of the `dataset_path` variable to the path wherein the downloaded information is saved. \n",
        "\n",
        "**Notes:** \n",
        "\n",
        "* The downloaded data can be obtained using the *TextAnalysis_DSTC2.ipynb* notebook.\n",
        "* Only a UC3M user can access to the download link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U57p__dC0n8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "18dc8046-92fd-4c88-ee79-8fccd41c8a1a"
      },
      "source": [
        "dataset_path = \"/content/drive/My Drive/_TFM/Notebooks/Datasets/\"\n",
        "logit_path = \"/content/drive/My Drive/_TFM/Notebooks/Models/logit/\"\n",
        "\n",
        "# Download some tools for the NLTK library\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZdTPM-2F_x_",
        "colab_type": "text"
      },
      "source": [
        "## Data reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXNpTM0d1qlA",
        "colab_type": "text"
      },
      "source": [
        "DSTC 2 dataset provides dialogues of a human talking to a information system labelled with slots and dialogue actions.\n",
        "\n",
        "In our case, the labels will be the users' intents which are the combination of acts and slots.\n",
        "\n",
        "The data only has information about users' utterances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFbOzzh910T5",
        "colab_type": "text"
      },
      "source": [
        "**Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R3w5Dgn1rQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train\n",
        "x_train_obj = open(dataset_path + \"x_train.obj\", \"rb\")\n",
        "x_train = pickle.load(x_train_obj)\n",
        "x_train_obj.close()\n",
        "\n",
        "# y_train\n",
        "y_train_obj = open(dataset_path + \"y_train.obj\", \"rb\")\n",
        "y_train = pickle.load(y_train_obj)\n",
        "y_train_obj.close()\n",
        "\n",
        "# x_valid\n",
        "x_valid_obj = open(dataset_path + \"x_valid.obj\", \"rb\")\n",
        "x_valid = pickle.load(x_valid_obj)\n",
        "x_valid_obj.close()\n",
        "\n",
        "# y_valid\n",
        "y_valid_obj = open(dataset_path + \"y_valid.obj\", \"rb\")\n",
        "y_valid = pickle.load(y_valid_obj)\n",
        "y_valid_obj.close()\n",
        "\n",
        "# x_test\n",
        "x_test_obj = open(dataset_path + \"x_test.obj\", \"rb\")\n",
        "x_test = pickle.load(x_test_obj)\n",
        "x_test_obj.close()\n",
        "\n",
        "# y_test\n",
        "y_test_obj = open(dataset_path + \"y_test.obj\", \"rb\")\n",
        "y_test = pickle.load(y_test_obj)\n",
        "y_test_obj.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1HON1gz1snx",
        "colab_type": "text"
      },
      "source": [
        "**Check data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlCX0-In1s64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "5b6c4705-3433-42f9-d112-65f0baaf4bc5"
      },
      "source": [
        "print(\"Data = (X: users' utterances, y: intents)\")\n",
        "\n",
        "print(\"\\n  > Train data:\")\n",
        "print(\"Samples: 8148 = %s\" % len(x_train))\n",
        "print(list(zip(x_train, y_train))[:3])\n",
        "\n",
        "print(\"\\n  > Validation data:\")\n",
        "print(\"Samples: 5656 = %s\" % len(x_valid))\n",
        "print(list(zip(x_valid, y_valid))[:3])\n",
        "\n",
        "print(\"\\n  > Test data:\")\n",
        "print(\"Samples: 5769 = %s\" % len(x_test))\n",
        "print(list(zip(x_test, y_test))[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data = (X: users' utterances, y: intents)\n",
            "\n",
            "  > Train data:\n",
            "Samples: 8148 = 8148\n",
            "[('cheap restaurant', ['inform_pricerange']), ('any', ['inform_this']), ('south', ['inform_area'])]\n",
            "\n",
            "  > Validation data:\n",
            "Samples: 5656 = 5656\n",
            "[('sil', ['unknown']), ('north part of town serving gastropub food', ['inform_food', 'inform_area']), ('north part of town serving gastropub food', ['inform_food', 'inform_area'])]\n",
            "\n",
            "  > Test data:\n",
            "Samples: 5769 = 5769\n",
            "[('uh yes im looking for a cheap restaurant in the west part of town', ['affirm', 'inform_pricerange', 'inform_area']), ('west', ['inform_area']), ('uh yes a cheap restaurant', ['affirm', 'inform_pricerange'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dXnolIPhii_",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCP9IzbT9p9B",
        "colab_type": "text"
      },
      "source": [
        "Each text sample will follow the next pipeline:\n",
        "\n",
        "*raw text* > **decontraction** > **normalization** > **lemmatization** > **lowercasing** > **tokenization** > *tokens*\n",
        "\n",
        "And then, we will build the vocabulary including the UNK token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqEgQTDn27B0",
        "colab_type": "text"
      },
      "source": [
        "**Settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvTyOo3q16yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge all samples\n",
        "x_data = np.concatenate([x_train, x_valid, x_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsdCz_Ba27kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decontract funtion\n",
        "contracted_words_obj = open(dataset_path + \"contracted_words.obj\", \"rb\")\n",
        "contracted_words = pickle.load(contracted_words_obj)\n",
        "contracted_words_obj.close()\n",
        "\n",
        "def decontract(text):\n",
        "  for word in text.split():\n",
        "    if word.lower() in contracted_words:\n",
        "      text = text.replace(word, contracted_words[word.lower()])\n",
        "  return text\n",
        "\n",
        "# Normalize function\n",
        "normalized_words_obj = open(dataset_path + \"normalized_words.obj\", \"rb\")\n",
        "normalized_words = pickle.load(normalized_words_obj)\n",
        "normalized_words_obj.close()\n",
        "\n",
        "def normalize(text):\n",
        "  for word in text.split():\n",
        "    if word.lower() in normalized_words:\n",
        "      text = text.replace(word, normalized_words[word.lower()])\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPmgQm3b2Bzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatize function\n",
        "\n",
        "# Set the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Get the POS tag for a given word\n",
        "def get_pos_tag(word):\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  tag_dict = {\"J\": wordnet.ADJ,\n",
        "              \"N\": wordnet.NOUN,\n",
        "              \"V\": wordnet.VERB,\n",
        "              \"R\": wordnet.ADV}\n",
        "  return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Lemmatize a text with POS tag\n",
        "def lemmatize(text, lemmatizer):\n",
        "  word_list = nltk.word_tokenize(text)\n",
        "  text_lemmatized = ' '.join([lemmatizer.lemmatize(word, get_pos_tag(word)) for word in word_list])\n",
        "  return text_lemmatized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZpUipOn-A3O",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbBmRY9v-BBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decontract, normalize, lemmatize, lowercase and tokenize the text samples\n",
        "tokenizer = Tokenizer(lower=True, oov_token=\"unk\")\n",
        "tokenizer.fit_on_texts(\n",
        "    [lemmatize(normalize(decontract(sample)), lemmatizer) for sample in x_data])\n",
        "\n",
        "# Vocabulary\n",
        "x_vocab = tokenizer.word_index\n",
        "x_vocab_size = len(x_vocab) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9UAu0Qj-6IE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "693b8b64-b705-4485-f8a1-4889695f9774"
      },
      "source": [
        "x_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "502"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpSIuoUS27vQ",
        "colab_type": "text"
      },
      "source": [
        "## Featurization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKwvb5Iq276Q",
        "colab_type": "text"
      },
      "source": [
        "### Tokens (TF-IDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIdgDqj828EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train\n",
        "x_train_eval = np.concatenate([x_train, x_valid])\n",
        "x_train_eval_tfidf = tokenizer.texts_to_matrix(\n",
        "    [lemmatize(normalize(decontract(sample)), lemmatizer) for sample in x_train_eval], mode=\"tfidf\")\n",
        "\n",
        "# x_test\n",
        "x_test_tfidf = tokenizer.texts_to_matrix(\n",
        "    [lemmatize(normalize(decontract(sample)), lemmatizer) for sample in x_test], mode=\"tfidf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODT-svKXV93F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "612211ba-c3ea-44c1-cc4c-4069011a3de6"
      },
      "source": [
        "np.shape(x_train_eval_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13804, 502)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwaRsKT1WMIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d6f7398-9b84-41d8-8df7-17337acfe800"
      },
      "source": [
        "np.shape(x_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5769, 502)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUMc2QxO3aQu",
        "colab_type": "text"
      },
      "source": [
        "### Labels (Multi-Label Binarizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2UjjpzE3aa0",
        "colab_type": "text"
      },
      "source": [
        "On the other hand, the labels will be encoded as *n* binary elements in an array, where *n* is the total number of labels.\n",
        "\n",
        "The binary vector (associated to each sample) indicates the presence of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjgX7Y17Ba3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the labels\n",
        "labels_obj = open(dataset_path + \"labels.obj\", \"rb\")\n",
        "labels = pickle.load(labels_obj)\n",
        "labels_obj.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsOwKVZQ3e2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the multi-label binarizer\n",
        "mlb = MultiLabelBinarizer(classes=labels)\n",
        "\n",
        "# y_train\n",
        "y_train_eval = np.concatenate([y_train, y_valid])\n",
        "y_train_eval_mlb = mlb.fit_transform(y_train_eval)\n",
        "\n",
        "# y_test\n",
        "y_test_mlb = mlb.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqFut7cQiGd2",
        "colab_type": "text"
      },
      "source": [
        "## Logistic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA7VUSdJXvwG",
        "colab_type": "text"
      },
      "source": [
        "In a multi-label scenario the Logistic Regression model must be wrapped in the OneVsRestClassifier to apply the One-vs-the-Rest (OvR) multi-label strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBezdhjkyhQ",
        "colab_type": "text"
      },
      "source": [
        "### Hyper-parameter tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPlS7isnr21q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "88068d78-9706-4b2a-e5f9-2480915c8429"
      },
      "source": [
        "# Define the model\n",
        "logit_model = OneVsRestClassifier(LogisticRegression(penalty=\"l1\", solver=\"liblinear\"), n_jobs=1)\n",
        "\n",
        "logit_model.get_params().keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['estimator__C', 'estimator__class_weight', 'estimator__dual', 'estimator__fit_intercept', 'estimator__intercept_scaling', 'estimator__l1_ratio', 'estimator__max_iter', 'estimator__multi_class', 'estimator__n_jobs', 'estimator__penalty', 'estimator__random_state', 'estimator__solver', 'estimator__tol', 'estimator__verbose', 'estimator__warm_start', 'estimator', 'n_jobs'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kIQW0FXxU0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the index of the train and validation data\n",
        "index_data = [i for i, _ in enumerate(x_train_eval)]\n",
        "index_train = index_data[:len(x_train)]\n",
        "index_valid = index_data[len(x_train):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GyDuM7_xhPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the hyper-parameters to tuning\n",
        "C = [1e-03, 0.1, 1.0, 2.0, 2.5]\n",
        "max_iter = [100, 120 ,140]\n",
        "param_grid = dict(estimator__C=C, estimator__max_iter=max_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmnGur-iteLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tuning through Grid Search approach\n",
        "logit_tune_model = GridSearchCV(logit_model,\n",
        "                                param_grid=param_grid,\n",
        "                                scoring = \"accuracy\",\n",
        "                                cv=[(index_train, index_valid)],\n",
        "                                verbose=1,\n",
        "                                n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8mx5E06r2rW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f0ece657-a25c-4dd8-9387-abd3d5bc0b51"
      },
      "source": [
        "# Select the model with the best hyper-parameters\n",
        "logit_tune_model = logit_tune_model.fit(x_train_eval_tfidf, y_train_eval_mlb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 15 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   13.3s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 27 is present in all training examples.\n",
            "  str(classes[c]))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOqzUipyCLqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ee98c8f8-94a9-492b-fef0-a81ed9f6087a"
      },
      "source": [
        "print(\"--- Hyper-paremeters selected ---\\n\")\n",
        "print(\"Best accuracy: %f using %s\" % (logit_tune_model.best_score_, logit_tune_model.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Hyper-paremeters selected ---\n",
            "\n",
            "Best accuracy: 0.963048 using {'estimator__C': 2.0, 'estimator__max_iter': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCQ1IpSVkROp",
        "colab_type": "text"
      },
      "source": [
        "**Save the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDn6F1zHkUSw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa6d08a9-fd5b-411e-d02b-d775a4a28a7d"
      },
      "source": [
        "dump(logit_tune_model, logit_path + \"logit_model.joblib\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/_TFM/Notebooks/Models/logit/logit_model.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqb6ENGfkUsT",
        "colab_type": "text"
      },
      "source": [
        "**Load the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yT8d0uqkYpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logit_tune_model = load(logit_path + \"logit_model.joblib\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF8RjoQflLKc",
        "colab_type": "text"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qx1TNhvL52R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction\n",
        "y_test_mlb_pred = logit_tune_model.predict(x_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N34ZDIMRk2lf",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYQZ_IoNCb5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4beec2c6-7cac-4c30-c977-e5bd336fac25"
      },
      "source": [
        "metrics.accuracy_score(y_test_mlb, y_test_mlb_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9580516553995493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65V1U8ualGRz",
        "colab_type": "text"
      },
      "source": [
        "**Precision, Recall & F1-Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64Mm3qh6emO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "693cff63-f575-4734-f791-2e7d0905435e"
      },
      "source": [
        "print(metrics.classification_report(y_test_mlb, y_test_mlb_pred, target_names=labels, zero_division=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "               ack       0.50      0.43      0.46         7\n",
            "            affirm       0.98      0.99      0.99       326\n",
            "               bye       1.00      1.00      1.00       619\n",
            "      confirm_area       0.67      1.00      0.80         2\n",
            "      confirm_food       0.76      0.76      0.76        25\n",
            "confirm_pricerange       0.78      1.00      0.88         7\n",
            "         deny_food       0.00      0.00      0.00         2\n",
            "         deny_name       0.00      0.00      0.00         0\n",
            "             hello       0.93      1.00      0.97        28\n",
            "       inform_area       1.00      0.99      0.99       653\n",
            "       inform_food       0.98      0.98      0.98      1439\n",
            "       inform_name       0.75      0.20      0.32        15\n",
            " inform_pricerange       1.00      0.99      1.00       526\n",
            "       inform_this       0.98      1.00      0.99       537\n",
            "            negate       0.96      0.93      0.94        69\n",
            "            repeat       1.00      0.53      0.69        19\n",
            "           reqalts       0.91      0.96      0.94       458\n",
            "           reqmore       0.00      0.00      0.00         0\n",
            "      request_addr       0.98      1.00      0.99       512\n",
            "      request_area       1.00      1.00      1.00        51\n",
            "      request_food       0.87      0.97      0.92       119\n",
            "     request_phone       1.00      1.00      1.00       511\n",
            "  request_postcode       1.00      0.99      1.00       112\n",
            "request_pricerange       0.99      1.00      0.99        79\n",
            "           restart       0.00      0.00      0.00         0\n",
            "          thankyou       0.99      1.00      0.99       606\n",
            " request_signature       0.00      0.00      0.00         0\n",
            "      request_name       0.00      0.00      0.00         4\n",
            "           unknown       0.93      0.96      0.94       503\n",
            "\n",
            "         micro avg       0.98      0.98      0.98      7229\n",
            "         macro avg       0.72      0.71      0.71      7229\n",
            "      weighted avg       0.97      0.98      0.98      7229\n",
            "       samples avg       0.97      0.98      0.98      7229\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI-30qoa7tcZ",
        "colab_type": "text"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQKtsMYb2zO4",
        "colab_type": "text"
      },
      "source": [
        "We inspect some predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzCa5XAo7xul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_pred = mlb.inverse_transform(y_test_mlb_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tTgHJFT7x5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ccaefd85-847b-4f81-def3-c501ec4b1b40"
      },
      "source": [
        "index_sample = 0\n",
        "\n",
        "print(\"User's utterance: %s\" % x_test[index_sample])\n",
        "\n",
        "print(\"\\nTrue label: %s\" % y_test[index_sample])\n",
        "print(\"Predicted label: %s\" % list(y_test_pred[index_sample]))\n",
        "\n",
        "print(\"\\nTrue binary label: %s\" % y_test_mlb[index_sample])\n",
        "print(\"Predicted binary label: %s\" % y_test_mlb_pred[index_sample])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User's utterance: uh yes im looking for a cheap restaurant in the west part of town\n",
            "\n",
            "True label: ['affirm', 'inform_pricerange', 'inform_area']\n",
            "Predicted label: ['affirm', 'inform_area', 'inform_pricerange']\n",
            "\n",
            "True binary label: [0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Predicted binary label: [0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}